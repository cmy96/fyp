{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:45.327009Z",
     "start_time": "2020-12-27T11:30:44.790181Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories\n",
    "Contains all the variable filepath names. If the filepaths are all set correctly the code should be able to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for patients clinical data\n",
    "FileToCheck = 'C:\\\\SMU_v2\\\\clinical_output.pkl'\n",
    "# File path to patients excel clinical data\n",
    "Clinical_Path = \"C:\\\\SMU_v2\\\\Clinical Data_Masked_v3_updated050220.xlsx\"\n",
    "# Checks for grouping\n",
    "group_file_path = \"C:\\\\SMU_v2\\\\model_dict\\\\\"\n",
    "# Saves file for layer\n",
    "layer_processing = \"C:\\\\SMU_v2\\\\clinical_full_data.csv\"\n",
    "# Checks for processed bills\n",
    "FileToCheck_Bills = 'C:\\\\SMU_v2\\\\bills_output.pkl'\n",
    "# Checks for processed prices\n",
    "FileToCheck_Price = \"C:\\\\SMU_v2\\\\price_master.pkl\"\n",
    "# Directory where all the billing data of the patients are stored\n",
    "path = str(\"C:\\\\SMU_v2\\\\\")\n",
    "# File path with the service code mappings\n",
    "service_code_mappings = \"C:\\\\SMU_v2\\\\service code mapping_smu.xlsx\"\n",
    "# Checks for processed time periods\n",
    "time_period = \"C:\\\\SMU_v2\\\\price_timeperiod.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:45.373467Z",
     "start_time": "2020-12-27T11:30:45.329969Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)\n",
    "\n",
    "def getDataToDF(xlws,start,end,first = False):\n",
    "    '''\n",
    "    input: excel worksheet path\n",
    "    start: rows to begin\n",
    "    end: rows to stop\n",
    "    first: used for when doing getting data using bacthing\n",
    "            true when the record is first wave\n",
    "            false when the record is after first wave\n",
    "    \n",
    "    1. years that are more than the current year are replaced into 19xx. eg 2020 => 1920\n",
    "    2. drop all dx date = 2020-01-01 (input errors)\n",
    "    3. drop all rows where dx_Date is empty\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    last_col = xlws.UsedRange.Columns.Count\n",
    "    header = xlws.Range(xlws.Cells(1, 1), xlws.Cells(1, last_col)).Value\n",
    "    content = xlws.Range(xlws.Cells(start, 1), xlws.Cells(end, last_col)).Value\n",
    "    \n",
    "    if first:\n",
    "        data = list(content[1:])\n",
    "    else:\n",
    "        data = list(content[0:])\n",
    "        \n",
    "    for x in range(0,len(data)):\n",
    "        data[x] = list(data[x])\n",
    "        for y in range(0,len(data[x])):\n",
    "            if isinstance(data[x][y], pywintypes.TimeType):\n",
    "                temp = str(data[x][y]).rstrip(\"+00:00\").strip()\n",
    "                if temp[:2] == '20' and int(temp[:4]) >= int(now.year):\n",
    "                    temp = '19' + temp[2:]\n",
    "                data[x][y] = datetime.datetime.strptime(temp, \"%Y-%m-%d\")\n",
    "    df = pd.DataFrame(data, columns=header[0])\n",
    "    \n",
    "    df.rename(columns={\"dob_new\":\"dob\",\"Age_new\":\"Age_@_Dx\"}, inplace = True)\n",
    "    \n",
    "    df.fillna(value=pd.np.nan, inplace=True)\n",
    "    \n",
    "    #drop all rows where dx_date is empty\n",
    "    df.drop(df[df[\"dx_date\"].isnull()].index, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def processCol(df):                \n",
    "            \n",
    "    #for those dropdown variables in dictionary\n",
    "    toDoDict = {}\n",
    "    toDoDict['Gender'] = [('1','2','4','5','6','9997','9998','9999'),\n",
    "                          ['Male','Female','Transsexual, NOS','Transsexual, natal male','Transsexual, natal female','NA',\n",
    "                           'Others (intersex, disorders of sexual development/DSD)','Unknown']]\n",
    "    toDoDict['c_tstage'] = [('1','2','3','4','5','6','7','8','9','11','12','13','41','42','43','44'),\n",
    "                            ['T1','T2','T3','T4','Tx','T0','Tis','T1mic','unknown','T1a','T1b','T1c',\\\n",
    "                             'T4a','T4b','T4c','T4d']]\n",
    "    toDoDict['cNstage'] = [('1','2','3','4','9','10','31','32','41','42','43'),\n",
    "                           ['N0','N1','N2','N3','Nx','unknown','N2a','N2b','N3a','N3b','N3c']]\n",
    "    toDoDict['cMstage'] = [('0','1','2','3','4','5','6','9997','9999'),\n",
    "                           ['MX','M0','M1','M1a','M1b','M1c','M2','NA','Unknown']]     \n",
    "    toDoDict['c_Staging'] = [('0','1','2','3','4','5','6','7','9','10','11','12','31','41'),\n",
    "                             ['DCIS/LCIS non-invasive','Stage 1','Stage 2A','Stage 2B',\\\n",
    "                              'Stage 3A','Stage 3B','Stage 3C','Stage 4','Unknown','Stage 0',\\\n",
    "                              'Stage 1A','Stage 1B','Stage 2','Stage 3']] \n",
    "    toDoDict['tstage'] = [('1','2','3','4','5','6','7','8','9','11','12','13','41','42','43','44'),\n",
    "                          ['T1','T2','T3','T4','Tx','T0','Tis','T1mic','unknown','T1a','T1b','T1c',\\\n",
    "                           'T4a','T4b','T4c','T4d']] \n",
    "    toDoDict['nstage'] = [('1','2','3','4','5','6','9','10','21','22','23','31','32','41','42','43'),\n",
    "                          ['N0','N1','N2','N3','N1mic','N0 (i+)','Nx','unknown','N1a','N1b','N1c',\\\n",
    "                           'N2a','N2b','N3a','N3b','N3c']]\n",
    "    toDoDict['Mstage'] = [('1','2','3','9'),('M0','M1','Mx','unknown')] \n",
    "    toDoDict['p_Staging'] = [('0','1','2','3','4','5','6','7','9','10','11','12','31','41'),\n",
    "                             ['DCIS/LCIS non-invasive','Stage 1','Stage 2A','Stage 2B',\\\n",
    "                              'Stage 3A','Stage 3B','Stage 3C','Stage 4','Unknown','Stage 0',\\\n",
    "                              'Stage 1A','Stage 1B','Stage 2','Stage 3']]\n",
    "    toDoDict['diff'] = [('0','1','2','3','9'),['grade 0','grade 1','grade 2','grade 3','unknown']] \n",
    "    toDoDict['TNM_Stage'] = [('0','1','2','3','4','5','6','7','9','10','11','12','13','31'),\n",
    "                             ['DCIS/LCIS non-invasive','stage 1','stage 2A','stage 2B',\\\n",
    "                              'stage 3A','stage 3B','stage 3C','stage 4','Unknown','Stage 0',\\\n",
    "                              'stage 1A','stage 1B','Stage 3','stage 2']]\n",
    "    toDoDict['ProgStage_AJCC8'] = [('0','4','11','12','21','22','31','32','33'),\n",
    "                                   ['Stage 0','Stage IV','Stage IA','Stage IB','Stage IIA',\\\n",
    "                                   'Stage IIB','Stage IIIA','Stage IIIB','Stage IIIC']] \n",
    "    toDoDict['ER'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']] \n",
    "    toDoDict['PR'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']]\n",
    "    toDoDict['cerbB2'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']]\n",
    "    toDoDict['Her2'] = [('1','2','3','4','9'),['Positive','Negative','Not Done','Equivocal','Unknown']]\n",
    "    toDoDict['cause_of_death'] = [('1','2','9'),['breast cancer related','N','unknown']]\n",
    "    toDoDict['Count_as_DFS'] = [('0','1'),['N' ,'RECURRENCE (any)']]\n",
    "    toDoDict['Count_as_OS'] = [('0','1'),['N' ,'Dead']]\n",
    "    \n",
    "    for q in list(['T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)','T','N','M','Stage']):\n",
    "        df[q] = df[q].str.lower()\n",
    "    \n",
    "    for k,v in toDoDict.items():\n",
    "        '''\n",
    "        1. convert numeric codes into category to standardise\n",
    "            k: column name\n",
    "            v[0]: numeric representation\n",
    "            v[1]: category\n",
    "        2. drop all those values that are outside of the defined numeric codes \n",
    "        '''\n",
    "        \n",
    "        v[1] = [x.lower() for x in v[1]]\n",
    "        df[k].replace(to_replace =v[0],\\\n",
    "                      value = v[1],\\\n",
    "                      inplace = True)\n",
    "        df[k] = df[k].str.lower()\n",
    "        \n",
    "        # allow null values\n",
    "        v[1].append(pd.np.nan)\n",
    "        \n",
    "        # drop abnormal values   \n",
    "        df.drop(df.loc[~df[k].isin(v[1])].index, inplace=True)\n",
    "    \n",
    "    DateList =  {\"death_age\":('death','dob')}\n",
    "    for k,v in DateList.items():\n",
    "        df[v[0]] = pd.to_datetime(df[v[0]])\n",
    "        df[v[1]] = pd.to_datetime(df[v[1]])\n",
    "        df[k] = (df[v[0]] - df[v[1]]).dt.days\n",
    "        df[k] = df[k].floordiv(365.2425 , fill_value = pd.np.nan) \n",
    "        \n",
    "        df = df.drop(columns=v[0])\n",
    "        \n",
    "        #drop negative age (input error)\n",
    "        df.drop(df[df[k] < 0].index, inplace=True)\n",
    "        \n",
    "    #drop all rows where death age is lesser than age @ dx\n",
    "    df.drop(df[df['death_age'] < df['Age_@_Dx']].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dropColCDM(df, listToDrop):\n",
    "    df = df.drop(columns=listToDrop)\n",
    "    return df\n",
    "\n",
    "def dataSetting(dropCol,FILE_FOLDER = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    function to read the pkl from from datasource\n",
    "        1. Remove dx_date that is NULL.\n",
    "        2. Drop all rows where crucial fields for X_features are NULL.\n",
    "        3. Convert Date columns into datetime format\n",
    "        4. Derive OS, CSS, DFS days based on dx_date\n",
    "        5. Create status column to indicate if the patient is dead or alive base on if death_age exists\n",
    "    '''\n",
    "    df = pd.read_pickle(FILE_FOLDER + \"clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "    to_drop = df[df['dx_date']==\"NA\"].index\n",
    "    df = drop_by_index(df,to_drop)\n",
    "\n",
    "    df.drop(columns=dropCol,inplace = True)\n",
    "\n",
    "    # drop all rows where dates are null\n",
    "    df.dropna(axis=0,\\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date','Age_@_Dx'],\\\n",
    "                    inplace=True)\n",
    "    \n",
    "    # convert all datetime in dataframe into dateime format for processing\n",
    "    df[\"Date_for_DFS\"] = pd.to_datetime(df[\"Date_for_DFS\"])\n",
    "    df[\"Date_for_OS\"] = pd.to_datetime(df[\"Date_for_OS\"])\n",
    "    df[\"Date_for_CSS\"] = pd.to_datetime(df[\"Date_for_CSS\"])\n",
    "    df[\"dx_date\"] = pd.to_datetime(df[\"dx_date\"])\n",
    "    df['last_seen']= pd.to_datetime(df[\"dx_date\"])\n",
    "    df['dob']= pd.to_datetime(df[\"dx_date\"])\n",
    "\n",
    "    # calculate in days\n",
    "    df[\"DFS_days\"] = (df[\"Date_for_DFS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"OS_days\"] = (df[\"Date_for_OS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"CSS_days\"] = (df[\"Date_for_CSS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "    # alive or dead\n",
    "    df['status'] = np.where(df['Count_as_OS'] == \"dead\", False, True)\n",
    "    \n",
    "#     # age\n",
    "#     df.loc[df['Age_@_Dx'].isnull() ,'Age_@_Dx'] = round((df['dx_date'] - df['dob']).dt.days/365.25,1)\n",
    "    #rename\n",
    "    df.rename(columns={\"p_Staging\":\"Stage\",\n",
    "                       \"c_tstage\":'T (no subgroup)', \n",
    "                       \"cNstage\":'N (no subgroup)', \n",
    "                       \"cMstage\":'M (no subgroup)',\n",
    "                       \"tstage\":'T', \n",
    "                       \"nstage\":'N', \n",
    "                       \"Mstage\":'M'},inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ComputeYears(df, Year_list):\n",
    "    '''\n",
    "    Create a list to contain df for different years of survival\n",
    "    The df will filter those patient that has deceased or days of survival longer than the defined years.\n",
    "    '''\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for i in Year_list:\n",
    "        tmp = {}\n",
    "        for x in list([\"DFS\", \"CSS\", \"OS\"]):\n",
    "            df['{}_{}_years'.format(x, i)] = np.where(\n",
    "                                                      np.logical_or(df['death_age'] > 0,\\\n",
    "                                                      df['{}_days'.format(x)]/(365.25*i) >= i),\\\n",
    "                                                      True,False)\n",
    "            tmp[x] = df[df['{}_{}_years'.format(x, i)] == True]\n",
    "        df_dict['{}_years'.format(i)] = tmp\n",
    "    return df_dict\n",
    "\n",
    "def settingXY(df, X_features, Y_features, OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    This function returns the X and Y features need for model training\n",
    "        - The function also generates one pkl that contains the One Hot Encoder for new raw data \n",
    "    \n",
    "    X_features = features to use for X\n",
    "    Y_features = features to use for Y \n",
    "    YEAR = years of patient record interested\n",
    "    SYTPE = survival type (OS, DFS, CSS)\n",
    "    OHE_LOCATION = location to store the pkl file\n",
    "    '''\n",
    "\n",
    "    X = df[X_features]\n",
    "    Y = df[Y_features]\n",
    "\n",
    "    # Save enconder so that we can OHE new data\n",
    "#     enc = OneHotEncoder()\n",
    "#     enc.fit(X)\n",
    "    \n",
    "    # OHE for probability\n",
    "#     X = enc.transform(X)\n",
    "#     with open(OHE_LOCATION + name + '_encoder.pickle', 'wb') as f:\n",
    "#         pickle.dump(enc, f) \n",
    "                  \n",
    "    # convert Y to structured array\n",
    "    s = Y.dtypes\n",
    "    Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "   \n",
    "    return X, Y\n",
    "def layeredData(df, group_dict,y_features, YEAR, STYPE):\n",
    "    \n",
    "    '''\n",
    "        this function generates the dataframe required for specific groups we hope to analyze\n",
    "        there are total 3 different groups but group 3 consist of multiple subgroups which leads a total of 5\n",
    "        dataframe.\n",
    "        Group 1: patient with stage 4 cancer\n",
    "        Group 2: patient which unknown records or at initial diagnosis stage\n",
    "        Group 3: make up of patient that does not belong to the groups above\n",
    "    '''\n",
    "    model_data_dict = {}\n",
    "    TO_USE = df['{}_years'.format(YEAR)][STYPE]\n",
    "    \n",
    "    print(\"Overall initial size: {} \\n\".format(TO_USE.shape[0]))\n",
    "    for key,value in group_dict.items():\n",
    "        TO_USE_COPY = TO_USE.copy()\n",
    "\n",
    "        tmp = {}\n",
    "        \n",
    "        waves = value['wave']\n",
    "    \n",
    "        if key != \"group 3\":\n",
    "            # for group 1 and group 2 select rows that contains either stage 4/non invasive in Stage\n",
    "            TO_USE_COPY = TO_USE_COPY.loc[TO_USE_COPY['Stage'] == group_dict[key]['stage'][0]]\n",
    "        else:\n",
    "            # for group 3 do not select rows that contains either stage 4 or non invasive in c_Staging or p_Staging\n",
    "            stage = np.logical_and(TO_USE_COPY['Stage'] != group_dict[key]['stage'][0],\\\n",
    "                                    TO_USE_COPY['Stage'] != group_dict[key]['stage'][1])\n",
    "            \n",
    "            TO_USE_COPY = TO_USE_COPY.loc[stage]\n",
    "            \n",
    "        print(\"{} data size: {}\".format(key,len(TO_USE_COPY)))\n",
    "        \n",
    "        for wave in waves:\n",
    "            \n",
    "            TO_USE_COPY2 = TO_USE_COPY.copy()\n",
    "            TO_USE_COPY2 = TO_USE_COPY2[waves[wave] + y_features]\n",
    "            \n",
    "            len_before = len(TO_USE_COPY2)\n",
    "            print(\"\\t{} data size before dropping nan: {}\".format(wave,len_before))\n",
    "            \n",
    "            TO_USE_COPY2.dropna(axis=0,subset=waves[wave]+ y_features, inplace=True)\n",
    "            TO_USE_COPY2.reset_index(drop=True)\n",
    "\n",
    "            len_after = len(TO_USE_COPY2)\n",
    "            print(\"\\t\\t after dropping nan: {}\".format(len_after))\n",
    "            \n",
    "            for i in waves[wave]:\n",
    "                if not (i in ['nodespos','Age_@_Dx','size_precise']):\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"category\")\n",
    "                else:\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"float32\")\n",
    "            \n",
    "            X, Y = settingXY(TO_USE_COPY2, waves[wave], y_features,name= \"{}_{}\".format(key,wave))   \n",
    "\n",
    "            tmp[wave] = {\n",
    "                            \"X\": X,\\\n",
    "                            \"Y\":Y      \n",
    "                        }    \n",
    "    \n",
    "        model_data_dict[key] = tmp\n",
    "    return model_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading of data files\n",
    "Here we check whether we have done this step before. If we have done this step before, the data will be saved to a pickle file defined in the FileToCheck variable. \n",
    "\n",
    "If we have not done this step before, we will take the excel file with the clinical data, and read it into a dataframe. Take note that as the excel file is password protected, we will have to manually input the password after the cell is run. After that, we do some preprocessing to clean the data. The preprocessing steps can be found in the processCol helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.661349Z",
     "start_time": "2020-12-27T11:30:45.375463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(FileToCheck):\n",
    "    CDM = pd.read_pickle(FileToCheck)\n",
    "else:\n",
    "    # primary set up\n",
    "    xlApp = win32com.client.Dispatch(\"Excel.Application\")\n",
    "    xlApp.Interactive = False\n",
    "    xlApp.Visible = False\n",
    "\n",
    "    #require user input for password\n",
    "    pwd = getpass.getpass('Enter file password: ')\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    xlwb = xlApp.Workbooks.Open(Clinical_Path, False, True, None, pwd)\n",
    "    xlws = xlwb.Worksheets(1) \n",
    "    last_row = xlws.UsedRange.Rows.Count\n",
    "\n",
    "    CDM = getDataToDF(xlws,1,last_row)\n",
    "\n",
    "    CDM = processCol(CDM)\n",
    "    \n",
    "    CDM['nodespos'].replace({'NA': pd.np.NaN}, inplace=True)\n",
    "    CDM['size_precise'].replace({'unknown': pd.np.NaN}, inplace=True)\n",
    "    \n",
    "    ##type casting to save space\n",
    "    col_list = list([\"Gender\",\"c_tstage\",\"cNstage\", \"cMstage\",\"c_Staging\",\"tstage\",\"nstage\",\"Mstage\",\\\n",
    "                     \"p_Staging\",\"diff\",\"TNM_Stage\",\"ProgStage_AJCC8\",\"ER\",\"PR\",\"cerbB2\",\\\n",
    "                     \"Her2\",\"cause_of_death\",\"Count_as_DFS\",\"Count_as_OS\",\"Count_as_CSS\",\\\n",
    "                     'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)',\\\n",
    "                     'T','N','M','Stage', 'Size'])\n",
    "    for i in col_list:\n",
    "        CDM.loc[:,i] = CDM[i].astype(\"category\")\n",
    "\n",
    "    CDM.loc[:,\"size_precise\"] = CDM[\"size_precise\"].astype(\"float32\")\n",
    "    CDM.loc[:,\"nodespos\"] = CDM[\"nodespos\"].astype(\"float16\")\n",
    "    \n",
    "    CDM.to_pickle(FileToCheck)\n",
    "    \n",
    "    #reset variables\n",
    "    xlws = None\n",
    "    xlwb.Close(False)\n",
    "    xlwb = None\n",
    "\n",
    "    #remove buffer and reset system settings\n",
    "    xlApp.Interactive = True\n",
    "    xlApp.Visible = True\n",
    "    xlApp.Quit()\n",
    "    xlApp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To process data for use with the layers approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 1_years, survival category: DFS, size: 22961\n",
      "Year: 1_years, survival category: CSS, size: 23435\n",
      "Year: 1_years, survival category: OS, size: 23435\n",
      "Year: 5_years, survival category: DFS, size: 6134\n",
      "Year: 5_years, survival category: CSS, size: 6268\n",
      "Year: 5_years, survival category: OS, size: 6268\n",
      "Year: 10_years, survival category: DFS, size: 5902\n",
      "Year: 10_years, survival category: CSS, size: 5902\n",
      "Year: 10_years, survival category: OS, size: 5902\n",
      "Overall initial size: 23435 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Stage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Stage'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a8e060683ba4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 }\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodel_data_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayeredData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYEAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-1020186d2851>\u001b[0m in \u001b[0;36mlayeredData\u001b[1;34m(df, group_dict, y_features, YEAR, STYPE)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"group 3\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;31m# for group 1 and group 2 select rows that contains either stage 4/non invasive in Stage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mTO_USE_COPY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTO_USE_COPY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTO_USE_COPY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Stage'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgroup_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;31m# for group 3 do not select rows that contains either stage 4 or non invasive in c_Staging or p_Staging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Stage'"
     ]
    }
   ],
   "source": [
    "listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH','END_OF_ENTRY']\n",
    "CDM = dataSetting(listToDrop)\n",
    "year_list = list([1,5,10])\n",
    "\n",
    "# only return data that has longer timeframe than the given interval\n",
    "df_dict = ComputeYears(CDM,year_list)\n",
    "\n",
    "# Display shape of data after filtering\n",
    "for i in df_dict: \n",
    "    for s_type in df_dict[i]:\n",
    "        print(\"Year: {}, survival category: {}, size: {}\".format(i,s_type,df_dict[i][s_type].shape[0]))\n",
    "        \n",
    "YEAR = 1\n",
    "STYPE = \"OS\"\n",
    "\n",
    "y_features = list(['status','OS_days'])\n",
    "\n",
    "group_dict = { \n",
    "                \"group 1\": {\n",
    "                             \"stage\": ['stage 4'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                     'T (no subgroup)', 'N (no subgroup)'],\\\n",
    "                                         \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N'],\\\n",
    "                                         \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise', 'nodespos']\n",
    "                                     }\n",
    "                           },\\\n",
    "                \"group 2\": {\n",
    "                             'stage': ['dcis/lcis non-invasive'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Size'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','size_precise']\n",
    "                                     }\n",
    "                           },\\\n",
    "                \"group 3\": {\n",
    "                             \"stage\": ['stage 4','dcis/lcis non-invasive'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                     'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)'],\\\n",
    "                                         \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N', 'M'],\\\n",
    "                                         \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise',\\\n",
    "                                                     'nodespos','M']\n",
    "                                     }\n",
    "                           },\n",
    "                }\n",
    "         \n",
    "model_data_dict = layeredData(df_dict, group_dict,y_features,YEAR, STYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.666337Z",
     "start_time": "2020-12-27T11:30:44.797Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ad6cf2f3bd8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_{}_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y{}{}_{}.pkl\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for group in model_data_dict:\n",
    "    for layer in model_data_dict[group]:\n",
    "        print(\"{}_{}_{}\".format(group,layer,model_data_dict[group][layer]['X'].shape))\n",
    "        model_data_dict[group][layer]['X'].to_pickle(\"{}{}_{}.pkl\".format(group_file_path,group,layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.667334Z",
     "start_time": "2020-12-27T11:30:44.799Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Stage', 'T', 'T (no subgroup)', 'M (no subgroup)', 'M', 'N (no subgroup)', 'N'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-48d5ff2eba5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;34m'Date_for_DFS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Date_for_OS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Date_for_CSS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;34m'Age_@_Dx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'death_age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'T (no subgroup)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'N (no subgroup)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'M (no subgroup)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m 'T','N','M','Stage']]\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_processing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2906\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2908\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;31m# we skip the warning on Categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Stage', 'T', 'T (no subgroup)', 'M (no subgroup)', 'M', 'N (no subgroup)', 'N'] not in index\""
     ]
    }
   ],
   "source": [
    "tmp = CDM[[\"Gender\",\"dx_date\",'diff','ProgStage_AJCC8','ER','PR','Her2',\\\n",
    "'size_precise','nodespos','cause_of_death',\\\n",
    "'Date_for_DFS','Date_for_OS','Date_for_CSS',\\\n",
    "'Age_@_Dx','death_age','status','T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)',\\\n",
    "'T','N','M','Stage']]\n",
    "\n",
    "tmp.to_csv(layer_processing,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bills\n",
    "The following section is for calculation of bills of a specific user. This is done by aggregating all the patients bills by year. \n",
    "\n",
    "Note that net present value was used to inflate prices to current year prices by given interest rate. Therefore all prices that we get are relevant to the year 2020 regardless of the year of the bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for use to calculate bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.668331Z",
     "start_time": "2020-12-27T11:30:44.800Z"
    }
   },
   "outputs": [],
   "source": [
    "MONTH = 30\n",
    "YEAR = 365\n",
    "current_year = 2020\n",
    "INTEREST = 0.03\n",
    "\n",
    "#calculate memory usage\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "def get_patient_bills(patient_id,bills_clean):\n",
    "    \"\"\"\n",
    "    input: patient id (str), all bills (dataframe)\n",
    "    output: A dataframe containing all bills of given patient\n",
    "    \"\"\"\n",
    "    subset = bills_clean[bills_clean[\"Patient.ID\"] == patient_id]\n",
    "    return subset\n",
    "\n",
    "    \n",
    "def get_cost_timeperiod(date, patient_bills):\n",
    "    \"\"\"\n",
    "    input: date(pd.Timestamp) and dataframe of patient's bills (1 patient only), last bill of patient\n",
    "    output: from date, calculate the sum of bills [6 months before, 6 months after, and yearly until 10 years later])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ##calculate NPV of prices\n",
    "        prices = patient_bills[\"Gross..exclude.GST.\"]*patient_bills[\"Service.Date.From.Date\"].map(lambda x: (1+INTEREST)**(current_year-x.year))\n",
    "        \n",
    "        ##calculate different groupings of dates\n",
    "        difference = patient_bills['Service.Date.From.Date'] - date\n",
    "        condition1 = difference.astype(\"timedelta64[D]\") > (-6*MONTH)\n",
    "        condition2 = difference.astype(\"timedelta64[D]\") >= 0 \n",
    "        condition3 = difference.astype(\"timedelta64[D]\") >= (6*MONTH) \n",
    "        condition4 = difference.astype(\"timedelta64[D]\") >= (1*YEAR) \n",
    "        condition5 = difference.astype(\"timedelta64[D]\") >= (2*YEAR) \n",
    "        condition6 = difference.astype(\"timedelta64[D]\") >= (3*YEAR) \n",
    "        condition7 = difference.astype(\"timedelta64[D]\") >= (4*YEAR) \n",
    "        condition8 = difference.astype(\"timedelta64[D]\") >= (5*YEAR) \n",
    "        condition9 = difference.astype(\"timedelta64[D]\") >= (6*YEAR) \n",
    "        condition10 = difference.astype(\"timedelta64[D]\") >= (7*YEAR) \n",
    "        condition11 = difference.astype(\"timedelta64[D]\") >= (8*YEAR) \n",
    "        condition12 = difference.astype(\"timedelta64[D]\") >= (9*YEAR) \n",
    "        condition13 = difference.astype(\"timedelta64[D]\") >= (10*YEAR) \n",
    "        before_6m = prices[condition1 & (condition2 == False)].sum()\n",
    "        after_6m = prices[condition2 & (condition3 == False)].sum()\n",
    "        after_1y = prices[condition3 & (condition4 == False)].sum()\n",
    "        after_2y = prices[condition4 & (condition5 == False)].sum()\n",
    "        after_3y = prices[condition5 & (condition6 == False)].sum()\n",
    "        after_4y = prices[condition6 & (condition7 == False)].sum()\n",
    "        after_5y = prices[condition7 & (condition8 == False)].sum()\n",
    "        after_6y = prices[condition8 & (condition9 == False)].sum()\n",
    "        after_7y = prices[condition9 & (condition10 == False)].sum()\n",
    "        after_8y = prices[condition10 & (condition11 == False)].sum()\n",
    "        after_9y = prices[condition11 & (condition12 == False)].sum()\n",
    "        after_10y = prices[condition12 & (condition13 == False)].sum()\n",
    "        \n",
    "        results = [before_6m, after_6m, after_1y, after_2y, after_3y, after_4y,\n",
    "               after_5y, after_6y, after_7y,after_8y, after_9y, after_10y]\n",
    "        \n",
    "        ##handle bills that definately cannot appear due to lack of data to differentiate it from no bills\n",
    "        data_limit = patient_bills[\"Service.Date.From.Date\"].max()\n",
    "        limit = data_limit - date\n",
    "        limit_int = abs(int(limit/np.timedelta64(1, 'Y')//1)) #abs not required but its there just in case\n",
    "        result_limit = results[:limit_int+3]\n",
    "        \n",
    "        while len(result_limit) < 12:\n",
    "            result_limit.append(np.NaN)\n",
    "        \n",
    "        return result_limit\n",
    "    except:\n",
    "        return [np.NaN for i in range(12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are millions of records to go through, we will slowly iterate through all the excel files and try to aggrerate the results such that we are left with one line of record for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.669328Z",
     "start_time": "2020-12-27T11:30:44.802Z"
    }
   },
   "outputs": [],
   "source": [
    "##read in the service code mappings\n",
    "mappings = pd.read_excel(service_code_mappings)\n",
    "\n",
    "if os.path.exists(FileToCheck):\n",
    "    bills_clean = pd.read_pickle(FileToCheck_Bills)\n",
    "else:\n",
    "    bills_clean = pd.DataFrame()\n",
    "    \n",
    "    # primary set up\n",
    "    xlApp = win32com.client.Dispatch(\"Excel.Application\")\n",
    "    xlApp.Interactive = False\n",
    "    xlApp.Visible = False\n",
    "\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    # Put files into dataframe dict\n",
    "    df_list = {}\n",
    "\n",
    "    #require user input for password\n",
    "    pwd = getpass.getpass('Enter file password: ')\n",
    "\n",
    "    # Pick out 'xlsx' files:\n",
    "    files_xls = ['Bills Data_10-12k (MASKED)v2.xlsx', 'Bills Data_12-14k (MASKED)v2.xlsx', 'Bills Data_14-16k (MASKED)v2.xlsx', 'Bills Data_16-18k (MASKED)v2.xlsx', 'Bills Data_18-20k (MASKED)v2.xlsx', 'Bills Data_1st 2k (MASKED)v2.xlsx', 'Bills Data_2-4k (MASKED)v2.xlsx', 'Bills Data_20-22k (MASKED)v2.xlsx', 'Bills Data_22-24k (MASKED)v2.xlsx', 'Bills Data_24-26k (MASKED)v2.xlsx', 'Bills Data_4-6k (MASKED)v2.xlsx', 'Bills Data_6-8k (MASKED)v2.xlsx', 'Bills Data_8-10k (MASKED)v2.xlsx', 'Bills Data_last 1k (MASKED)v2.xlsx']\n",
    "\n",
    "    for f in files_xls:\n",
    "        \n",
    "        #reading of data\n",
    "        first = True\n",
    "        counter = 1\n",
    "        xlwb = xlApp.Workbooks.Open(path+f, False, True, None, pwd)\n",
    "        xlws = xlwb.Worksheets(1) \n",
    "#         last_row = 500\n",
    "        last_row = xlws.UsedRange.Rows.Count\n",
    "        last_col = xlws.UsedRange.Columns.Count\n",
    "        n=50000\n",
    "        while counter < last_row:\n",
    "            print(\"Processing {}\".format(counter))\n",
    "            prev_counter = counter\n",
    "            counter = min(counter+n,last_row)\n",
    "            content = xlws.Range(xlws.Cells(prev_counter, 1), xlws.Cells(counter, last_col)).Value\n",
    "            if first:\n",
    "                print(last_row)\n",
    "                first = False\n",
    "                col_headers = content[0]\n",
    "                data = list(content[1:])\n",
    "            else:\n",
    "                data = list(content[0:])\n",
    "            for x in range(0,len(data)):\n",
    "                data[x] = list(data[x])\n",
    "                for y in range(0,len(data[x])):\n",
    "                    if isinstance(data[x][y], pywintypes.TimeType):\n",
    "                        temp = str(data[x][y]).rstrip(\"+00:00\").strip()\n",
    "                        data[x][y] = datetime.datetime.strptime(temp, \"%Y-%m-%d\")\n",
    "\n",
    "            bills = pd.DataFrame(data, columns=col_headers)\n",
    "            #cleaning of data\n",
    "            ##remove all bills with gross cost of NA\n",
    "            bills['Gross..exclude.GST.'].replace(\"NA\",np.nan,inplace=True)\n",
    "            bills = bills.dropna(subset=['Gross..exclude.GST.'])\n",
    "\n",
    "            ##removal of several unused columns\n",
    "            unused = [\"Gender\",\"Date.of.Birth\",\"Net..exclude.GST.\",\"Net.Payable\",\"Service.Cost\",\\\n",
    "                      \"Billed.Qty\",\"Service.Entered.Price\",\"Doctor.Surcharge..SVC.\",\"Total.Cost\",\\\n",
    "                      \"TOSP\",\"Billing.Category.Description\",\"Billing.Status.Description\",\"Billing.Date\",\\\n",
    "                      \"Admit.Sub.Specialty.Description\",\"Admit.Specialty.Description\",\\\n",
    "                      \"Admit.Accommodation.Category.Description\",\"Diagnosis.Description..ICD10.\",\\\n",
    "                      \"Diagnosis.Description\"]\n",
    "            \n",
    "            bills = bills.drop(unused,axis = 1)\n",
    "\n",
    "            ##can consider removing these columns \n",
    "#             unused = [\"Service.Category.1.Code\", \"Service.Category.1.Description\",\"Service.Category.2.Code\",\"Service.Category.2.Description\",\n",
    "#                      \"Service.Summary.Code\"]\n",
    "#             bills = bills.drop(unused,axis = 1)\n",
    "\n",
    "            ##replace all expected unknown with np.nan\n",
    "            bills = bills.replace(\"Expected Unknown\",np.nan)\n",
    "            bills = bills.replace(\"EXPUNKNOWN\",np.nan)\n",
    "\n",
    "            ##add the processed bills to a clean df\n",
    "            \n",
    "            bills_clean = bills_clean.append(bills.reset_index())\n",
    "            \n",
    "        name = xlApp.ActiveWorkbook.Name\n",
    "        #reset variables\n",
    "        xlws = None\n",
    "        xlwb.Close(False)\n",
    "        xlwb = None\n",
    "        \n",
    "        ##type casting to save space\n",
    "        bills_clean.loc[:,\"Gross..exclude.GST.\"] = bills_clean[\"Gross..exclude.GST.\"].astype(\"uint64\")\n",
    "        bills_clean.loc[:,\"Service.Qty\"] = bills_clean[\"Service.Qty\"].astype(\"uint16\")\n",
    "        bills_clean.loc[:,\"Institution.Code\"] = bills_clean[\"Institution.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Summary..Description\"] = bills_clean[\"Service.Summary..Description\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Code\"] = bills_clean[\"Service.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Short.Text\"] = bills_clean[\"Service.Short.Text\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Department.Description\"] = bills_clean[\"Service.Department.Description\"].astype('category')\n",
    "        bills_clean.loc[:,\"Diagnosis.Code\"] = bills_clean[\"Diagnosis.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Diagnosis.Code..ICD10.\"] = bills_clean[\"Diagnosis.Code..ICD10.\"].astype('category')\n",
    "\n",
    "    #remove buffer and reset system settings\n",
    "    xlApp.Interactive = True\n",
    "    xlApp.Visible = True\n",
    "    xlApp.Quit()\n",
    "    xlApp = None\n",
    "\n",
    "    bills_clean = bills_clean.drop([\"index\"],axis = 1)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    ##save the df\n",
    "    outToPickle(bills_clean,FileToCheck_Bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.669328Z",
     "start_time": "2020-12-27T11:30:44.803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(FileToCheck_Price):\n",
    "    bills_master = pd.read_pickle(FileToCheck_Price)\n",
    "else:\n",
    "    prices = []\n",
    "    counter = 1\n",
    "    for i,j in CDM.iterrows():\n",
    "        counter+=1\n",
    "        prices.append(\n",
    "            get_cost_timeperiod(j[\"dx_date\"],\n",
    "                     get_patient_bills(j[\"NRIC\"],bills_clean)))\n",
    "        if counter%10 == 0:\n",
    "            print(counter)\n",
    "        if counter%1000 == 0:\n",
    "            with open(\"price_processing\", 'wb') as fp:\n",
    "                pickle.dump(prices, fp)\n",
    "\n",
    "    bills_master = pd.DataFrame(prices,columns=[\"before_6m\", \"after_6m\", \"after_1y\", \"after_2y\", \"after_3y\", \"after_4y\",\n",
    "               \"after_5y\", \"after_6y\", \"after_7y\",\"after_8y\", \"after_9y\", \"after_10y\"])\n",
    "    bills_master.to_pickle(FileToCheck_Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge between Bills and Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.670325Z",
     "start_time": "2020-12-27T11:30:44.805Z"
    }
   },
   "outputs": [],
   "source": [
    "CDM = CDM.reset_index(drop=True)\n",
    "bills_master = bills_master.reset_index(drop=True)\n",
    "combined = pd.concat([CDM,bills_master],axis=1)\n",
    "\n",
    "\n",
    "if os.path.exists(time_period):\n",
    "    pass\n",
    "else:\n",
    "    \n",
    "    bills_processed_time = combined.dropna(axis=0, \\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date',\\\n",
    "                            'Age_@_Dx','size_precise', 'nodespos'],\n",
    "                    )\n",
    "    bills_processed_time.to_pickle(time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
