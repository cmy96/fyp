{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:45.327009Z",
     "start_time": "2020-12-27T11:30:44.790181Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:45.373467Z",
     "start_time": "2020-12-27T11:30:45.329969Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)\n",
    "\n",
    "def getDataToDF(xlws,start,end,first = False):\n",
    "    '''\n",
    "    input: excel worksheet path\n",
    "    start: rows to begin\n",
    "    end: rows to stop\n",
    "    first: used for when doing getting data using bacthing\n",
    "            true when the record is first wave\n",
    "            false when the record is after first wave\n",
    "    \n",
    "    1. years that are more than the current year are replaced into 19xx. eg 2020 => 1920\n",
    "    2. drop all dx date = 2020-01-01 (input errors)\n",
    "    3. drop all rows where dx_Date is empty\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    last_col = xlws.UsedRange.Columns.Count\n",
    "    header = xlws.Range(xlws.Cells(1, 1), xlws.Cells(1, last_col)).Value\n",
    "    content = xlws.Range(xlws.Cells(start, 1), xlws.Cells(end, last_col)).Value\n",
    "    \n",
    "    if first:\n",
    "        data = list(content[1:])\n",
    "    else:\n",
    "        data = list(content[0:])\n",
    "        \n",
    "    for x in range(0,len(data)):\n",
    "        data[x] = list(data[x])\n",
    "        for y in range(0,len(data[x])):\n",
    "            if isinstance(data[x][y], pywintypes.TimeType):\n",
    "                temp = str(data[x][y]).rstrip(\"+00:00\").strip()\n",
    "                if temp[:2] == '20' and int(temp[:4]) >= int(now.year):\n",
    "                    temp = '19' + temp[2:]\n",
    "                data[x][y] = datetime.datetime.strptime(temp, \"%Y-%m-%d\")\n",
    "    df = pd.DataFrame(data, columns=header[0])\n",
    "    \n",
    "    df.rename(columns={\"dob_new\":\"dob\",\"Age_new\":\"Age_@_Dx\"}, inplace = True)\n",
    "    \n",
    "    df.fillna(value=pd.np.nan, inplace=True)\n",
    "    \n",
    "    #drop all rows where dx_date is empty\n",
    "    df.drop(df[df[\"dx_date\"].isnull()].index, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def processCol(df):                \n",
    "            \n",
    "    #for those dropdown variables in dictionary\n",
    "    toDoDict = {}\n",
    "    toDoDict['Gender'] = [('1','2','4','5','6','9997','9998','9999'),\n",
    "                          ['Male','Female','Transsexual, NOS','Transsexual, natal male','Transsexual, natal female','NA',\n",
    "                           'Others (intersex, disorders of sexual development/DSD)','Unknown']]\n",
    "    toDoDict['c_tstage'] = [('1','2','3','4','5','6','7','8','9','11','12','13','41','42','43','44'),\n",
    "                            ['T1','T2','T3','T4','Tx','T0','Tis','T1mic','unknown','T1a','T1b','T1c',\\\n",
    "                             'T4a','T4b','T4c','T4d']]\n",
    "    toDoDict['cNstage'] = [('1','2','3','4','9','10','31','32','41','42','43'),\n",
    "                           ['N0','N1','N2','N3','Nx','unknown','N2a','N2b','N3a','N3b','N3c']]\n",
    "    toDoDict['cMstage'] = [('0','1','2','3','4','5','6','9997','9999'),\n",
    "                           ['MX','M0','M1','M1a','M1b','M1c','M2','NA','Unknown']]     \n",
    "    toDoDict['c_Staging'] = [('0','1','2','3','4','5','6','7','9','10','11','12','31','41'),\n",
    "                             ['DCIS/LCIS non-invasive','Stage 1','Stage 2A','Stage 2B',\\\n",
    "                              'Stage 3A','Stage 3B','Stage 3C','Stage 4','Unknown','Stage 0',\\\n",
    "                              'Stage 1A','Stage 1B','Stage 2','Stage 3']] \n",
    "    toDoDict['tstage'] = [('1','2','3','4','5','6','7','8','9','11','12','13','41','42','43','44'),\n",
    "                          ['T1','T2','T3','T4','Tx','T0','Tis','T1mic','unknown','T1a','T1b','T1c',\\\n",
    "                           'T4a','T4b','T4c','T4d']] \n",
    "    toDoDict['nstage'] = [('1','2','3','4','5','6','9','10','21','22','23','31','32','41','42','43'),\n",
    "                          ['N0','N1','N2','N3','N1mic','N0 (i+)','Nx','unknown','N1a','N1b','N1c',\\\n",
    "                           'N2a','N2b','N3a','N3b','N3c']]\n",
    "    toDoDict['Mstage'] = [('1','2','3','9'),('M0','M1','Mx','unknown')] \n",
    "    toDoDict['p_Staging'] = [('0','1','2','3','4','5','6','7','9','10','11','12','31','41'),\n",
    "                             ['DCIS/LCIS non-invasive','Stage 1','Stage 2A','Stage 2B',\\\n",
    "                              'Stage 3A','Stage 3B','Stage 3C','Stage 4','Unknown','Stage 0',\\\n",
    "                              'Stage 1A','Stage 1B','Stage 2','Stage 3']]\n",
    "    toDoDict['diff'] = [('0','1','2','3','9'),['grade 0','grade 1','grade 2','grade 3','unknown']] \n",
    "    toDoDict['TNM_Stage'] = [('0','1','2','3','4','5','6','7','9','10','11','12','13','31'),\n",
    "                             ['DCIS/LCIS non-invasive','stage 1','stage 2A','stage 2B',\\\n",
    "                              'stage 3A','stage 3B','stage 3C','stage 4','Unknown','Stage 0',\\\n",
    "                              'stage 1A','stage 1B','Stage 3','stage 2']]\n",
    "    toDoDict['ProgStage_AJCC8'] = [('0','4','11','12','21','22','31','32','33'),\n",
    "                                   ['Stage 0','Stage IV','Stage IA','Stage IB','Stage IIA',\\\n",
    "                                   'Stage IIB','Stage IIIA','Stage IIIB','Stage IIIC']] \n",
    "    toDoDict['ER'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']] \n",
    "    toDoDict['PR'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']]\n",
    "    toDoDict['cerbB2'] = [('1','2','3','4'),['positive','negative','unknown','Equivocal']]\n",
    "    toDoDict['Her2'] = [('1','2','3','4','9'),['Positive','Negative','Not Done','Equivocal','Unknown']]\n",
    "    toDoDict['cause_of_death'] = [('1','2','9'),['breast cancer related','N','unknown']]\n",
    "    toDoDict['Count_as_DFS'] = [('0','1'),['N' ,'RECURRENCE (any)']]\n",
    "    toDoDict['Count_as_OS'] = [('0','1'),['N' ,'Dead']]\n",
    "    \n",
    "    for q in list(['T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)','T','N','M','Stage']):\n",
    "        df[q] = df[q].str.lower()\n",
    "    \n",
    "    for k,v in toDoDict.items():\n",
    "        '''\n",
    "        1. convert numeric codes into category to standardise\n",
    "            k: column name\n",
    "            v[0]: numeric representation\n",
    "            v[1]: category\n",
    "        2. drop all those values that are outside of the defined numeric codes \n",
    "        '''\n",
    "        \n",
    "        v[1] = [x.lower() for x in v[1]]\n",
    "        df[k].replace(to_replace =v[0],\\\n",
    "                      value = v[1],\\\n",
    "                      inplace = True)\n",
    "        df[k] = df[k].str.lower()\n",
    "        \n",
    "        # allow null values\n",
    "        v[1].append(pd.np.nan)\n",
    "        \n",
    "        # drop abnormal values   \n",
    "        df.drop(df.loc[~df[k].isin(v[1])].index, inplace=True)\n",
    "    \n",
    "    DateList =  {\"death_age\":('death','dob')}\n",
    "    for k,v in DateList.items():\n",
    "        df[v[0]] = pd.to_datetime(df[v[0]])\n",
    "        df[v[1]] = pd.to_datetime(df[v[1]])\n",
    "        df[k] = (df[v[0]] - df[v[1]]).dt.days\n",
    "        df[k] = df[k].floordiv(365.2425 , fill_value = pd.np.nan) \n",
    "        \n",
    "        df = df.drop(columns=v[0])\n",
    "        \n",
    "        #drop negative age (input error)\n",
    "        df.drop(df[df[k] < 0].index, inplace=True)\n",
    "        \n",
    "    #drop all rows where death age is lesser than age @ dx\n",
    "    df.drop(df[df['death_age'] < df['Age_@_Dx']].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dropColCDM(df, listToDrop):\n",
    "    df = df.drop(columns=listToDrop)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.661349Z",
     "start_time": "2020-12-27T11:30:45.375463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FileToCheck = 'C:\\\\SMU_v2\\\\clinical_output.pkl'\n",
    "Clinical_Path = \"C:\\\\SMU_v2\\\\Clinical Data_Masked_v3_updated050220.xlsx\"\n",
    "if os.path.exists(FileToCheck):\n",
    "    CDM = pd.read_pickle(FileToCheck)\n",
    "else:\n",
    "    # primary set up\n",
    "    xlApp = win32com.client.Dispatch(\"Excel.Application\")\n",
    "    xlApp.Interactive = False\n",
    "    xlApp.Visible = False\n",
    "\n",
    "    #require user input for password\n",
    "    pwd = getpass.getpass('Enter file password: ')\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    xlwb = xlApp.Workbooks.Open(Clinical_Path, False, True, None, pwd)\n",
    "    xlws = xlwb.Worksheets(1) \n",
    "    last_row = xlws.UsedRange.Rows.Count\n",
    "\n",
    "    CDM = getDataToDF(xlws,1,last_row)\n",
    "\n",
    "    CDM = processCol(CDM)\n",
    "    \n",
    "    CDM['nodespos'].replace({'NA': pd.np.NaN}, inplace=True)\n",
    "    CDM['size_precise'].replace({'unknown': pd.np.NaN}, inplace=True)\n",
    "    \n",
    "    ##type casting to save space\n",
    "    col_list = list([\"Gender\",\"c_tstage\",\"cNstage\", \"cMstage\",\"c_Staging\",\"tstage\",\"nstage\",\"Mstage\",\\\n",
    "                     \"p_Staging\",\"diff\",\"TNM_Stage\",\"ProgStage_AJCC8\",\"ER\",\"PR\",\"cerbB2\",\\\n",
    "                     \"Her2\",\"cause_of_death\",\"Count_as_DFS\",\"Count_as_OS\",\"Count_as_CSS\",\\\n",
    "                     'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)',\\\n",
    "                     'T','N','M','Stage', 'Size'])\n",
    "    for i in col_list:\n",
    "        CDM.loc[:,i] = CDM[i].astype(\"category\")\n",
    "\n",
    "    CDM.loc[:,\"size_precise\"] = CDM[\"size_precise\"].astype(\"float32\")\n",
    "    CDM.loc[:,\"nodespos\"] = CDM[\"nodespos\"].astype(\"float16\")\n",
    "    \n",
    "    CDM.to_pickle(FileToCheck)\n",
    "    \n",
    "    #reset variables\n",
    "    xlws = None\n",
    "    xlwb.Close(False)\n",
    "    xlwb = None\n",
    "\n",
    "    #remove buffer and reset system settings\n",
    "    xlApp.Interactive = True\n",
    "    xlApp.Visible = True\n",
    "    xlApp.Quit()\n",
    "    xlApp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.664341Z",
     "start_time": "2020-12-27T11:30:44.794Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataSetting(dropCol,FILE_FOLDER = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    function to read the pkl from from datasource\n",
    "        1. Remove dx_date that is NULL.\n",
    "        2. Drop all rows where crucial fields for X_features are NULL.\n",
    "        3. Convert Date columns into datetime format\n",
    "        4. Derive OS, CSS, DFS days based on dx_date\n",
    "        5. Create status column to indicate if the patient is dead or alive base on if death_age exists\n",
    "    '''\n",
    "    df = pd.read_pickle(FILE_FOLDER + \"clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "    to_drop = df[df['dx_date']==\"NA\"].index\n",
    "    df = drop_by_index(df,to_drop)\n",
    "\n",
    "    df.drop(columns=dropCol,inplace = True)\n",
    "\n",
    "    # drop all rows where dates are null\n",
    "    df.dropna(axis=0,\\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date','Age_@_Dx'],\\\n",
    "                    inplace=True)\n",
    "    \n",
    "    # convert all datetime in dataframe into dateime format for processing\n",
    "    df[\"Date_for_DFS\"] = pd.to_datetime(df[\"Date_for_DFS\"])\n",
    "    df[\"Date_for_OS\"] = pd.to_datetime(df[\"Date_for_OS\"])\n",
    "    df[\"Date_for_CSS\"] = pd.to_datetime(df[\"Date_for_CSS\"])\n",
    "    df[\"dx_date\"] = pd.to_datetime(df[\"dx_date\"])\n",
    "    df['last_seen']= pd.to_datetime(df[\"dx_date\"])\n",
    "    df['dob']= pd.to_datetime(df[\"dx_date\"])\n",
    "\n",
    "    # calculate in days\n",
    "    df[\"DFS_days\"] = (df[\"Date_for_DFS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"OS_days\"] = (df[\"Date_for_OS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"CSS_days\"] = (df[\"Date_for_CSS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "    # alive or dead\n",
    "    df['status'] = np.where(df['Count_as_OS'] == \"dead\", False, True)\n",
    "    \n",
    "#     # age\n",
    "#     df.loc[df['Age_@_Dx'].isnull() ,'Age_@_Dx'] = round((df['dx_date'] - df['dob']).dt.days/365.25,1)\n",
    "    \n",
    "    return df\n",
    "def ComputeYears(df, Year_list):\n",
    "    '''\n",
    "    Create a list to contain df for different years of survival\n",
    "    The df will filter those patient that has deceased or days of survival longer than the defined years.\n",
    "    '''\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for i in Year_list:\n",
    "        tmp = {}\n",
    "        for x in list([\"DFS\", \"CSS\", \"OS\"]):\n",
    "            df['{}_{}_years'.format(x, i)] = np.where(\n",
    "                                                      np.logical_or(df['death_age'] > 0,\\\n",
    "                                                      df['{}_days'.format(x)]/(365.25*i) >= i),\\\n",
    "                                                      True,False)\n",
    "            tmp[x] = df[df['{}_{}_years'.format(x, i)] == True]\n",
    "        df_dict['{}_years'.format(i)] = tmp\n",
    "    return df_dict\n",
    "def settingXY(df, X_features, Y_features, OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    This function returns the X and Y features need for model training\n",
    "        - The function also generates one pkl that contains the One Hot Encoder for new raw data \n",
    "    \n",
    "    X_features = features to use for X\n",
    "    Y_features = features to use for Y \n",
    "    YEAR = years of patient record interested\n",
    "    SYTPE = survival type (OS, DFS, CSS)\n",
    "    OHE_LOCATION = location to store the pkl file\n",
    "    '''\n",
    "    import pickle\n",
    "    from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "    X = df[X_features]\n",
    "    Y = df[Y_features]\n",
    "\n",
    "    # Save enconder so that we can OHE new data\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    \n",
    "    # OHE for probability\n",
    "    X = enc.transform(X)\n",
    "    with open(OHE_LOCATION + name + '_encoder.pickle', 'wb') as f:\n",
    "        pickle.dump(enc, f) \n",
    "                  \n",
    "    # convert Y to structured array\n",
    "    s = Y.dtypes\n",
    "    Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "   \n",
    "    return X, Y\n",
    "def layeredData(df, group_dict,y_features, YEAR, STYPE):\n",
    "    \n",
    "    '''\n",
    "        this function generates the dataframe required for specific groups we hope to analyze\n",
    "        there are total 3 different groups but group 3 consist of multiple subgroups which leads a total of 5\n",
    "        dataframe.\n",
    "        Group 1: patient with stage 4 cancer\n",
    "        Group 2: patient which unknown records or at initial diagnosis stage\n",
    "        Group 3: make up of patient that does not belong to the groups above\n",
    "    '''\n",
    "    model_data_dict = {}\n",
    "    TO_USE = df['{}_years'.format(YEAR)][STYPE]\n",
    "    \n",
    "    print(\"Overall initial size: {} \\n\".format(TO_USE.shape[0]))\n",
    "        \n",
    "    for key,value in group_dict.items():\n",
    "        TO_USE_COPY = TO_USE.copy()\n",
    "\n",
    "        tmp = {}\n",
    "        \n",
    "        waves = value['wave']\n",
    "    \n",
    "        if key != \"group 3\":\n",
    "            # for group 1 and group 2 select rows that contains either stage 4/non invasive in Stage\n",
    "            TO_USE_COPY = TO_USE_COPY.loc[TO_USE_COPY['Stage'] == group_dict[key]['stage'][0]]\n",
    "        else:\n",
    "            # for group 3 do not select rows that contains either stage 4 or non invasive in c_Staging or p_Staging\n",
    "            stage = np.logical_and(TO_USE_COPY['Stage'] != group_dict[key]['stage'][0],\\\n",
    "                                    TO_USE_COPY['Stage'] != group_dict[key]['stage'][1])\n",
    "            \n",
    "            TO_USE_COPY = TO_USE_COPY.loc[stage]\n",
    "            \n",
    "        print(\"{} data size: {}\".format(key,len(TO_USE_COPY)))\n",
    "        \n",
    "        for wave in waves:\n",
    "            TO_USE_COPY2 = TO_USE_COPY.copy()\n",
    "            TO_USE_COPY2 = TO_USE_COPY2[waves[wave] + y_features]\n",
    "            \n",
    "            len_before = len(TO_USE_COPY2)\n",
    "            print(\"\\t{} data size before dropping nan: {}\".format(wave,len_before))\n",
    "            \n",
    "            TO_USE_COPY2.dropna(axis=0,subset=waves[wave]+ y_features, inplace=True)\n",
    "            TO_USE_COPY2.reset_index(drop=True)\n",
    "\n",
    "            len_after = len(TO_USE_COPY2)\n",
    "            print(\"\\t\\t after dropping nan: {}\".format(len_after))\n",
    "            \n",
    "            for i in waves[wave]:\n",
    "                if not (i in ['nodespos','Age_@_Dx','size_precise']):\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"category\")\n",
    "                else:\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"float32\")\n",
    "            \n",
    "            X, Y = settingXY(TO_USE_COPY2, waves[wave], y_features,name= \"{}_{}\".format(key,wave))   \n",
    "\n",
    "            tmp[wave] = {\n",
    "                            \"X\": X,\\\n",
    "                            \"Y\":Y      \n",
    "                        }    \n",
    "    \n",
    "        model_data_dict[key] = tmp\n",
    "    return model_data_dict\n",
    "\n",
    "listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH','END_OF_ENTRY']\n",
    "CDM = dataSetting(listToDrop)\n",
    "year_list = list([1,5,10])\n",
    "\n",
    "# only return data that has longer timeframe than the given interval\n",
    "df_dict = ComputeYears(CDM,year_list)\n",
    "\n",
    "# Display shape of data after filtering\n",
    "for i in df_dict: \n",
    "    for s_type in df_dict[i]:\n",
    "        print(\"Year: {}, survival category: {}, size: {}\".format(i,s_type,df_dict[i][s_type].shape[0]))\n",
    "        \n",
    "YEAR = 1\n",
    "STYPE = \"OS\"\n",
    "\n",
    "y_features = list(['status','OS_days'])\n",
    "\n",
    "group_dict = { \n",
    "                \"group 1\": {\n",
    "                             \"stage\": ['stage 4'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                     'T (no subgroup)', 'N (no subgroup)'],\\\n",
    "                                         \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N'],\\\n",
    "                                         \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise', 'nodespos']\n",
    "                                     }\n",
    "                           },\\\n",
    "                \"group 2\": {\n",
    "                             'stage': ['dcis/lcis non-invasive'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Size'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','size_precise']\n",
    "                                     }\n",
    "                           },\\\n",
    "                \"group 3\": {\n",
    "                             \"stage\": ['stage 4','dcis/lcis non-invasive'],\\\n",
    "                             'wave': {\n",
    "                                         \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                         \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                     'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)'],\\\n",
    "                                         \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N', 'M'],\\\n",
    "                                         \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise',\\\n",
    "                                                     'nodespos','M']\n",
    "                                     }\n",
    "                           },\n",
    "                }\n",
    "         \n",
    "model_data_dict = layeredData(df_dict, group_dict,y_features,YEAR, STYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.666337Z",
     "start_time": "2020-12-27T11:30:44.797Z"
    }
   },
   "outputs": [],
   "source": [
    "for group in model_data_dict:\n",
    "    for layer in model_data_dict[group]:\n",
    "        print(\"{}_{}_{}\".format(group,layer,model_data_dict[group][layer]['X'].shape))\n",
    "        model_data_dict[group][layer]['X'].to_pickle(\"C:\\\\SMU_v2\\\\model_dict\\\\{}_{}.pkl\".format(group,layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.667334Z",
     "start_time": "2020-12-27T11:30:44.799Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = CDM[[\"Gender\",\"dx_date\",'diff','ProgStage_AJCC8','ER','PR','Her2',\\\n",
    "'size_precise','nodespos','cause_of_death',\\\n",
    "'Date_for_DFS','Date_for_OS','Date_for_CSS',\\\n",
    "'Age_@_Dx','death_age','status','T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)',\\\n",
    "'T','N','M','Stage']]\n",
    "\n",
    "tmp.to_csv(\"C:\\\\SMU_v2\\\\clinical_full_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.668331Z",
     "start_time": "2020-12-27T11:30:44.800Z"
    }
   },
   "outputs": [],
   "source": [
    "MONTH = 30\n",
    "YEAR = 365\n",
    "current_year = 2020\n",
    "INTEREST = 0.03\n",
    "\n",
    "#calculate memory usage\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "def get_patient_bills(patient_id,bills_clean):\n",
    "    \"\"\"\n",
    "    input: patient id (str), all bills (dataframe)\n",
    "    output: A dataframe containing all bills of given patient\n",
    "    \"\"\"\n",
    "    subset = bills_clean[bills_clean[\"Patient.ID\"] == patient_id]\n",
    "    return subset\n",
    "\n",
    "    \n",
    "def get_cost_timeperiod(date, patient_bills):\n",
    "    \"\"\"\n",
    "    input: date(pd.Timestamp) and dataframe of patient's bills (1 patient only), last bill of patient\n",
    "    output: from date, calculate the sum of bills [6 months before, 6 months after, and yearly until 10 years later])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ##calculate NPV of prices\n",
    "        prices = patient_bills[\"Gross..exclude.GST.\"]*patient_bills[\"Service.Date.From.Date\"].map(lambda x: (1+INTEREST)**(current_year-x.year))\n",
    "        \n",
    "        ##calculate different groupings of dates\n",
    "        difference = patient_bills['Service.Date.From.Date'] - date\n",
    "        condition1 = difference.astype(\"timedelta64[D]\") > (-6*MONTH)\n",
    "        condition2 = difference.astype(\"timedelta64[D]\") >= 0 \n",
    "        condition3 = difference.astype(\"timedelta64[D]\") >= (6*MONTH) \n",
    "        condition4 = difference.astype(\"timedelta64[D]\") >= (1*YEAR) \n",
    "        condition5 = difference.astype(\"timedelta64[D]\") >= (2*YEAR) \n",
    "        condition6 = difference.astype(\"timedelta64[D]\") >= (3*YEAR) \n",
    "        condition7 = difference.astype(\"timedelta64[D]\") >= (4*YEAR) \n",
    "        condition8 = difference.astype(\"timedelta64[D]\") >= (5*YEAR) \n",
    "        condition9 = difference.astype(\"timedelta64[D]\") >= (6*YEAR) \n",
    "        condition10 = difference.astype(\"timedelta64[D]\") >= (7*YEAR) \n",
    "        condition11 = difference.astype(\"timedelta64[D]\") >= (8*YEAR) \n",
    "        condition12 = difference.astype(\"timedelta64[D]\") >= (9*YEAR) \n",
    "        condition13 = difference.astype(\"timedelta64[D]\") >= (10*YEAR) \n",
    "        before_6m = prices[condition1 & (condition2 == False)].sum()\n",
    "        after_6m = prices[condition2 & (condition3 == False)].sum()\n",
    "        after_1y = prices[condition3 & (condition4 == False)].sum()\n",
    "        after_2y = prices[condition4 & (condition5 == False)].sum()\n",
    "        after_3y = prices[condition5 & (condition6 == False)].sum()\n",
    "        after_4y = prices[condition6 & (condition7 == False)].sum()\n",
    "        after_5y = prices[condition7 & (condition8 == False)].sum()\n",
    "        after_6y = prices[condition8 & (condition9 == False)].sum()\n",
    "        after_7y = prices[condition9 & (condition10 == False)].sum()\n",
    "        after_8y = prices[condition10 & (condition11 == False)].sum()\n",
    "        after_9y = prices[condition11 & (condition12 == False)].sum()\n",
    "        after_10y = prices[condition12 & (condition13 == False)].sum()\n",
    "        \n",
    "        results = [before_6m, after_6m, after_1y, after_2y, after_3y, after_4y,\n",
    "               after_5y, after_6y, after_7y,after_8y, after_9y, after_10y]\n",
    "        \n",
    "        ##handle bills that definately cannot appear due to lack of data to differentiate it from no bills\n",
    "        data_limit = patient_bills[\"Service.Date.From.Date\"].max()\n",
    "        limit = data_limit - date\n",
    "        limit_int = abs(int(limit/np.timedelta64(1, 'Y')//1)) #abs not required but its there just in case\n",
    "        result_limit = results[:limit_int+3]\n",
    "        \n",
    "        while len(result_limit) < 12:\n",
    "            result_limit.append(np.NaN)\n",
    "        \n",
    "        return result_limit\n",
    "    except:\n",
    "        return [np.NaN for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.669328Z",
     "start_time": "2020-12-27T11:30:44.802Z"
    }
   },
   "outputs": [],
   "source": [
    "FileToCheck = 'C:\\\\SMU_v2\\\\bills_output.pkl'\n",
    "\n",
    "##read in the service code mappings\n",
    "mappings = pd.read_excel(\"C:\\\\SMU_v2\\\\service code mapping_smu.xlsx\")\n",
    "\n",
    "if os.path.exists(FileToCheck):\n",
    "    bills_clean = pd.read_pickle(FileToCheck)\n",
    "else:\n",
    "    bills_clean = pd.DataFrame()\n",
    "    \n",
    "    # primary set up\n",
    "    xlApp = win32com.client.Dispatch(\"Excel.Application\")\n",
    "    xlApp.Interactive = False\n",
    "    xlApp.Visible = False\n",
    "\n",
    "    path = str(\"C:\\\\SMU_v2\\\\\")\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    # Put files into dataframe dict\n",
    "    df_list = {}\n",
    "\n",
    "    #require user input for password\n",
    "    pwd = getpass.getpass('Enter file password: ')\n",
    "\n",
    "    # Pick out 'xlsx' files:\n",
    "    files_xls = ['Bills Data_10-12k (MASKED)v2.xlsx', 'Bills Data_12-14k (MASKED)v2.xlsx', 'Bills Data_14-16k (MASKED)v2.xlsx', 'Bills Data_16-18k (MASKED)v2.xlsx', 'Bills Data_18-20k (MASKED)v2.xlsx', 'Bills Data_1st 2k (MASKED)v2.xlsx', 'Bills Data_2-4k (MASKED)v2.xlsx', 'Bills Data_20-22k (MASKED)v2.xlsx', 'Bills Data_22-24k (MASKED)v2.xlsx', 'Bills Data_24-26k (MASKED)v2.xlsx', 'Bills Data_4-6k (MASKED)v2.xlsx', 'Bills Data_6-8k (MASKED)v2.xlsx', 'Bills Data_8-10k (MASKED)v2.xlsx', 'Bills Data_last 1k (MASKED)v2.xlsx']\n",
    "\n",
    "    for f in files_xls:\n",
    "        \n",
    "        #reading of data\n",
    "        first = True\n",
    "        counter = 1\n",
    "        xlwb = xlApp.Workbooks.Open(path+f, False, True, None, pwd)\n",
    "        xlws = xlwb.Worksheets(1) \n",
    "#         last_row = 500\n",
    "        last_row = xlws.UsedRange.Rows.Count\n",
    "        last_col = xlws.UsedRange.Columns.Count\n",
    "        n=50000\n",
    "        while counter < last_row:\n",
    "            print(\"Processing {}\".format(counter))\n",
    "            prev_counter = counter\n",
    "            counter = min(counter+n,last_row)\n",
    "            content = xlws.Range(xlws.Cells(prev_counter, 1), xlws.Cells(counter, last_col)).Value\n",
    "            if first:\n",
    "                print(last_row)\n",
    "                first = False\n",
    "                col_headers = content[0]\n",
    "                data = list(content[1:])\n",
    "            else:\n",
    "                data = list(content[0:])\n",
    "            for x in range(0,len(data)):\n",
    "                data[x] = list(data[x])\n",
    "                for y in range(0,len(data[x])):\n",
    "                    if isinstance(data[x][y], pywintypes.TimeType):\n",
    "                        temp = str(data[x][y]).rstrip(\"+00:00\").strip()\n",
    "                        data[x][y] = datetime.datetime.strptime(temp, \"%Y-%m-%d\")\n",
    "\n",
    "            bills = pd.DataFrame(data, columns=col_headers)\n",
    "            #cleaning of data\n",
    "            ##remove all bills with gross cost of NA\n",
    "            bills['Gross..exclude.GST.'].replace(\"NA\",np.nan,inplace=True)\n",
    "            bills = bills.dropna(subset=['Gross..exclude.GST.'])\n",
    "\n",
    "            ##removal of several unused columns\n",
    "            unused = [\"Gender\",\"Date.of.Birth\",\"Net..exclude.GST.\",\"Net.Payable\",\"Service.Cost\",\\\n",
    "                      \"Billed.Qty\",\"Service.Entered.Price\",\"Doctor.Surcharge..SVC.\",\"Total.Cost\",\\\n",
    "                      \"TOSP\",\"Billing.Category.Description\",\"Billing.Status.Description\",\"Billing.Date\",\\\n",
    "                      \"Admit.Sub.Specialty.Description\",\"Admit.Specialty.Description\",\\\n",
    "                      \"Admit.Accommodation.Category.Description\",\"Diagnosis.Description..ICD10.\",\\\n",
    "                      \"Diagnosis.Description\"]\n",
    "            \n",
    "            bills = bills.drop(unused,axis = 1)\n",
    "\n",
    "            ##can consider removing these columns \n",
    "#             unused = [\"Service.Category.1.Code\", \"Service.Category.1.Description\",\"Service.Category.2.Code\",\"Service.Category.2.Description\",\n",
    "#                      \"Service.Summary.Code\"]\n",
    "#             bills = bills.drop(unused,axis = 1)\n",
    "\n",
    "            ##replace all expected unknown with np.nan\n",
    "            bills = bills.replace(\"Expected Unknown\",np.nan)\n",
    "            bills = bills.replace(\"EXPUNKNOWN\",np.nan)\n",
    "\n",
    "            ##add the processed bills to a clean df\n",
    "            \n",
    "            bills_clean = bills_clean.append(bills.reset_index())\n",
    "            \n",
    "        name = xlApp.ActiveWorkbook.Name\n",
    "        #reset variables\n",
    "        xlws = None\n",
    "        xlwb.Close(False)\n",
    "        xlwb = None\n",
    "        \n",
    "        ##type casting to save space\n",
    "        bills_clean.loc[:,\"Gross..exclude.GST.\"] = bills_clean[\"Gross..exclude.GST.\"].astype(\"uint64\")\n",
    "        bills_clean.loc[:,\"Service.Qty\"] = bills_clean[\"Service.Qty\"].astype(\"uint16\")\n",
    "        bills_clean.loc[:,\"Institution.Code\"] = bills_clean[\"Institution.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Summary..Description\"] = bills_clean[\"Service.Summary..Description\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Code\"] = bills_clean[\"Service.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Short.Text\"] = bills_clean[\"Service.Short.Text\"].astype('category')\n",
    "        bills_clean.loc[:,\"Service.Department.Description\"] = bills_clean[\"Service.Department.Description\"].astype('category')\n",
    "        bills_clean.loc[:,\"Diagnosis.Code\"] = bills_clean[\"Diagnosis.Code\"].astype('category')\n",
    "        bills_clean.loc[:,\"Diagnosis.Code..ICD10.\"] = bills_clean[\"Diagnosis.Code..ICD10.\"].astype('category')\n",
    "\n",
    "    #remove buffer and reset system settings\n",
    "    xlApp.Interactive = True\n",
    "    xlApp.Visible = True\n",
    "    xlApp.Quit()\n",
    "    xlApp = None\n",
    "\n",
    "    bills_clean = bills_clean.drop([\"index\"],axis = 1)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    ##save the df\n",
    "    outToPickle(bills_clean,FileToCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.669328Z",
     "start_time": "2020-12-27T11:30:44.803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FileToCheck = \"C:\\\\SMU_v2\\\\price_master.pkl\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "if os.path.exists(FileToCheck):\n",
    "    bills_master = pd.read_pickle(FileToCheck)\n",
    "else:\n",
    "    prices = []\n",
    "    counter = 1\n",
    "    for i,j in CDM.iterrows():\n",
    "        counter+=1\n",
    "        prices.append(\n",
    "            get_cost_timeperiod(j[\"dx_date\"],\n",
    "                     get_patient_bills(j[\"NRIC\"],bills_clean)))\n",
    "        if counter%10 == 0:\n",
    "            print(counter)\n",
    "        if counter%1000 == 0:\n",
    "            with open(\"price_processing\", 'wb') as fp:\n",
    "                pickle.dump(prices, fp)\n",
    "\n",
    "    bills_master = pd.DataFrame(prices,columns=[\"before_6m\", \"after_6m\", \"after_1y\", \"after_2y\", \"after_3y\", \"after_4y\",\n",
    "               \"after_5y\", \"after_6y\", \"after_7y\",\"after_8y\", \"after_9y\", \"after_10y\"])\n",
    "    bills_master.to_pickle(FileToCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge between Bills and Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:30:47.670325Z",
     "start_time": "2020-12-27T11:30:44.805Z"
    }
   },
   "outputs": [],
   "source": [
    "CDM = CDM.reset_index(drop=True)\n",
    "bills_master = bills_master.reset_index(drop=True)\n",
    "print(CDM.shape)\n",
    "print(bills_master.shape)\n",
    "combined = pd.concat([CDM,bills_master],axis=1)\n",
    "FileToCheck = \"C:\\\\SMU_v2\\\\price_timeperiod.pkl\"\n",
    "\n",
    "\n",
    "if os.path.exists(FileToCheck):\n",
    "    bills_processed_time = pd.read_pickle(FileToCheck)\n",
    "else:\n",
    "    \n",
    "    bills_processed_time = combined.dropna(axis=0, \\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date',\\\n",
    "                            'Age_@_Dx','size_precise', 'nodespos'],\n",
    "                    )\n",
    "    bills_processed_time.to_pickle(FileToCheck)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
