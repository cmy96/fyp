{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "#ann model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the pickle file that we created in the preprocessing step\n",
    "\n",
    "master_path = \"C:\\\\SMU_v2\\\\price_timeperiod.pkl\"\n",
    "\n",
    "OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\"\n",
    "\n",
    "saved_model_group1_10y = 'C:\\\\SMU_v2\\\\ann\\\\model_group1_10y.h5'\n",
    "saved_model_group1_5y = 'C:\\\\SMU_v2\\\\ann\\\\model_group1_5y.h5'\n",
    "saved_model_group1_2y = 'C:\\\\SMU_v2\\\\ann\\\\model_group1_2y.h5'\n",
    "saved_model_group1_1y = 'C:\\\\SMU_v2\\\\ann\\\\model_group1_1y.h5'\n",
    "saved_model_group1_6m = 'C:\\\\SMU_v2\\\\ann\\\\model_group1_6m.h5'\n",
    "\n",
    "saved_model_group2_10y = 'C:\\\\SMU_v2\\\\ann\\\\model_group2_10y.h5'\n",
    "saved_model_group2_5y = 'C:\\\\SMU_v2\\\\ann\\\\model_group2_5y.h5'\n",
    "saved_model_group2_2y = 'C:\\\\SMU_v2\\\\ann\\\\model_group2_2y.h5'\n",
    "saved_model_group2_1y = 'C:\\\\SMU_v2\\\\ann\\\\model_group2_1y.h5'\n",
    "saved_model_group2_6m = 'C:\\\\SMU_v2\\\\ann\\\\model_group2_6m.h5'\n",
    "\n",
    "saved_model_group3_10y = 'C:\\\\SMU_v2\\\\ann\\\\model_group3_10y.h5'\n",
    "saved_model_group3_5y = 'C:\\\\SMU_v2\\\\ann\\\\model_group3_5y.h5'\n",
    "saved_model_group3_2y = 'C:\\\\SMU_v2\\\\ann\\\\model_group3_2y.h5'\n",
    "saved_model_group3_1y = 'C:\\\\SMU_v2\\\\ann\\\\model_group3_1y.h5'\n",
    "saved_model_group3_6m = 'C:\\\\SMU_v2\\\\ann\\\\model_group3_6m.h5'\n",
    "\n",
    "\n",
    "best_layer_group1 = \"C:\\\\SMU_v2\\\\Layered Folder\\\\group 1_layer 4.pkl\"\n",
    "best_layer_group2 = \"C:\\\\SMU_v2\\\\Layered Folder\\\\group 2_layer 1.pkl\"\n",
    "best_layer_group3 = \"C:\\\\SMU_v2\\\\Layered Folder\\\\group 3_layer 5.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN\n",
    "We read in the data and user a artificial neural network to output the prices that the patient will be spending [6months before, 6 months after, 1 year after, 2 year after, 5 year after] with reference to the patient's diagnosis date.\n",
    "The input data will be the clinical data that we gotten from the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    Returns all the data that needs to be used for ANN.\n",
    "    Output(3 dataframes): clinical data, clinical data (OHE), bills grouped by time period\n",
    "    \"\"\"\n",
    "    master = pd.read_pickle(master_path)\n",
    "    listToKeep = ['NRIC','dx_date','tstage','nstage', 'Mstage', 'ER', 'PR',\\\n",
    "               'Her2', 'size_precise', 'nodespos', 'Age_@_Dx']\n",
    "    \n",
    "    clinical = master[listToKeep]\n",
    "    \n",
    "    OHE = [i for i in clinical.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    x_clinical = pd.get_dummies(clinical,columns=OHE,dummy_na=True).reset_index(drop=True)\n",
    "    prices_grouped = master[[\"NRIC\",\"before_6m\", \"after_6m\", \"after_1y\", \"after_2y\", \"after_3y\", \"after_4y\",\n",
    "               \"after_5y\", \"after_6y\", \"after_7y\",\"after_8y\", \"after_9y\", \"after_10y\"]]\n",
    "    return clinical.reset_index(drop=True), x_clinical.reset_index(drop=True), prices_grouped.reset_index(drop=True)\n",
    "\n",
    "def scale_data(data,scale_obj):\n",
    "    \"\"\"\n",
    "    transforms then scales data according to min-max\n",
    "    \"\"\"\n",
    "#     data = data.apply(np.log1p)\n",
    "    prices_grouped_scaled = pd.DataFrame(scale_obj.fit_transform(data))\n",
    "    return prices_grouped_scaled\n",
    "\n",
    "def scale_data_reverse(data,scale_obj):\n",
    "    \"\"\"\n",
    "    returns a dataframe that reverses the min-max that was done previously\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(scale_obj.inverse_transform(data))\n",
    "#     predictions_scaled_reverse = data.apply(np.expm1)\n",
    "    return data\n",
    "\n",
    "def ann_structure(input_shape,output_units):\n",
    "    \"\"\"\n",
    "    function to declare ANN structure. just for code cleaniness\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64, input_shape=(input_shape,)))         # input layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(64, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    \n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layerr\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layerr\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(output_units, activation=tf.nn.leaky_relu))   # one output layer with 1 outputs\n",
    "    return model\n",
    "\n",
    "def remove_out_of_range(data):\n",
    "    \"\"\"\n",
    "    determines index of data where there is no additional information\n",
    "    \"\"\"\n",
    "    y1 = data[data[\"after_1y\"].isnull()].index\n",
    "    y2 = data[data[\"after_2y\"].isnull()].index\n",
    "    y5 = data[data[\"after_5y\"].isnull()].index\n",
    "    y10 = data[data[\"after_10y\"].isnull()].index\n",
    "    return {\n",
    "        \"y1\":[4,y1], \n",
    "        \"y2\":[5,y2], \n",
    "        \"y5\":[8,y5], \n",
    "        \"y10\":[13,y10]}\n",
    "\n",
    "def remove_meaningless_data(data):\n",
    "    \"\"\"\n",
    "    returns index of all rows that do not add any additional input. aka all fields are 0\n",
    "    \"\"\"\n",
    "    return data[data.sum(axis=1)==0].index\n",
    "\n",
    "def drop_by_index(X,y,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    y = y.drop(indexes)\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    return(X,y)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 100:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "    \n",
    "def process_time_period(data,scope):\n",
    "    \"\"\"\n",
    "    Takes in yearly healthcare costs of patients and processes it into 1,2,5,10 year values\n",
    "    \"\"\"\n",
    "    y = pd.DataFrame()\n",
    "    y[\"6 months before\"] = data.iloc[:,0]\n",
    "    y[\"6 months after\"] = data.iloc[:,1]\n",
    "    y[\"1 year after\"] = data.iloc[:,2]\n",
    "    if scope != \"y1\":\n",
    "        y[\"2 years after\"] = data.iloc[:,3]\n",
    "        if scope != \"y2\":\n",
    "            y[\"5 years after\"] = data.iloc[:,4:7].sum(axis=1)\n",
    "            if scope != \"y5\":\n",
    "                y[\"10 years after\"] = data.iloc[:,7:].sum(axis=1)\n",
    "    return y\n",
    "    \n",
    "def make_prediction(all_users,user,model,mms):\n",
    "    \"\"\"\n",
    "    Given user data(dataframe) and the trained model, outputs the predicted values.\n",
    "    \n",
    "    Only works if all items in new user data has appeared at least once before\n",
    "    \"\"\"\n",
    "    all_users = all_users.reset_index().drop(columns=\"index\")\n",
    "    last_row = all_users.shape[0]\n",
    "    all_users = all_users.append(user)\n",
    "    all_users = all_users.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "    OHE = [i for i in all_users.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    usersOHE = pd.get_dummies(all_users,columns=OHE,dummy_na=True).reset_index().drop(columns=\"index\")\n",
    "    prediction_x = usersOHE\n",
    "#     print(prediction_x.shape)\n",
    "    pred = model.predict(prediction_x)\n",
    "    predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(pred),columns=[\"6 months before\",\"6 months after\",\n",
    "                                                                                   \"1 year after\",\"2 years after\",\n",
    "                                                                                   \"5 years after\",\"10 years after\"][:pred.shape[1]])\n",
    "    return pd.DataFrame([pd.DataFrame(predictions_scaled_reverse).iloc[last_row]]).reset_index().drop(columns=\"index\")\n",
    "\n",
    "def make_comparison(all_users,all_users_OHE,NRIC,bills,model,mms):\n",
    "    \"\"\"\n",
    "    Given a specific user, calculate out his actual cost and predicted costs\n",
    "    \"\"\"\n",
    "    x = all_users[all_users[\"NRIC\"] == NRIC]\n",
    "    pred = make_prediction(all_users,x,model,mms)\n",
    "    pred[\"Status\"] = \"Prediction\"\n",
    "    if pred.shape[1] == 7:\n",
    "        limit = 0\n",
    "    elif pred.shape[1] == 6:\n",
    "        limit = 1\n",
    "    else:\n",
    "        limit = 2\n",
    "    y_test = process_time_period(bills[all_users[\"NRIC\"] == NRIC],limit)\n",
    "    y_test[\"Status\"] = \"True data\"\n",
    "    y_test.columns = [\"6 months before\",\"6 months after\",\"1 year after\",\n",
    "                      \"2 years after\",\"5 years after\",\"10 years after\"][:pred.shape[1]-1] +[\"Status\"]\n",
    "    \n",
    "    to_return = y_test.append(pred)\n",
    "    return to_return\n",
    "\n",
    "def get_percentage(df1,df2,percentage):\n",
    "    \"\"\"\n",
    "    Given 2 dataframes, get the difference between the dataframes, \n",
    "    and return number of records that fall within a given percentage.\n",
    "    Eg: df1 contains 5 values [1,2,3,4,5]. Df2 contains [1,2,9,4,5]. 4 out of 5 values in \n",
    "    df1 fall within +- 5(percentage)% of the values in the same postion in df2. \n",
    "    Thus function will return 4/5 or 0.8\n",
    "    \"\"\"\n",
    "    process = lambda s1,s2: abs(s1-s2)/s2 < percentage \n",
    "    combined = df1.combine(df2, process)\n",
    "    total_count = (df1.shape[0] * df1.shape[1])\n",
    "    minus = sum([pd.value_counts(df2[i].values)[0] for i in df2.columns])\n",
    "    total_count -= minus\n",
    "    return combined.sum().sum() / total_count\n",
    "\n",
    "def display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse):\n",
    "    graph = pd.DataFrame(np.arange(0,2,.01),columns=[\"Percentage\"])\n",
    "    graph[\"viz\"] = graph.applymap(lambda x: get_percentage(predictions_scaled_reverse,y_test_scaled_reverse,x))\n",
    "    show = graph.plot.area(x=\"Percentage\")\n",
    "    show.set_title(\"Model performance ({})\".format(scope))\n",
    "    show.set_xlabel(\"Percentage Difference from Ground Truth\")\n",
    "    show.set_ylabel(\"Percentage of all our predictions\")\n",
    "    show.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    show.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    \n",
    "def get_data(scope,clinicalOHE, bills_grouped, outlier=False):\n",
    "    index = remove_indexes[scope]\n",
    "\n",
    "    X = clinicalOHE\n",
    "    y = bills_grouped.iloc[:,:index[0]]  \n",
    "\n",
    "    X = X.iloc[:,2:]\n",
    "    y = process_time_period(y.iloc[:,1:],scope)\n",
    "\n",
    "    print(\"Data shape original: {}\".format(X.shape[0]))\n",
    "    X,y = drop_by_index(X,y,index[1])\n",
    "\n",
    "    print(\"Data shape removing data out of scope: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "    meaningless = remove_meaningless_data(y)\n",
    "    X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "    print(\"Data shape meaningless data: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if outlier:\n",
    "        clf = IsolationForest(contamination=\"auto\",behaviour=\"new\",random_state=42)\n",
    "        out = clf.fit_predict(y)\n",
    "        out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "        remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "        X,y = drop_by_index(X,y,remove)\n",
    "\n",
    "        print(\"Data shape after removing outliers: {}\".format(X.shape[0]))\n",
    "\n",
    "    mms = MinMaxScaler()\n",
    "    y_scaled = scale_data(y,mms)\n",
    "    return X,y_scaled,mms\n",
    "\n",
    "def loadOHE(df,OHE_LOCATION = OHE_LOCATION, name=\"\"):\n",
    "    '''\n",
    "    load enconder to OHE new raw data for prediction\n",
    "    '''\n",
    "    with open( \"{}{}{}\".format(OHE_LOCATION, name, '_encoder.pickle'), 'rb') as f:\n",
    "        enc = pickle.load(f) \n",
    "    \n",
    "    #type case object to category\n",
    "    if len(list(df.select_dtypes(include=[object]).columns)) > 0:\n",
    "        typeCastList = list(df.select_dtypes(include=[object]).columns)\n",
    "        df[typeCastList] = df[typeCastList].astype(\"category\")\n",
    "    OHE_New_Data = enc.transform(df)\n",
    "    \n",
    "    return OHE_New_Data\n",
    "\n",
    "def get_running_total(data):\n",
    "    \"\"\"\n",
    "    Change the data from additional costs into running total\n",
    "    \"\"\"\n",
    "    data = data.copy(deep=True)\n",
    "    data1 = data.copy(deep=True)\n",
    "    plot_attr_list = ['after_6m','after_1y','after_2y','after_3y','after_4y','after_5y','after_6y','after_7y',\"after_8y\",\"after_9y\",\"after_10y\"]\n",
    "    for i in range(len(plot_attr_list)-1,-1,-1):\n",
    "        current_attr = plot_attr_list[i]\n",
    "        data.loc[:,current_attr] = data.iloc[:,:i+2].sum(axis=1)\n",
    "        data.loc[data1[current_attr].isna(),current_attr] = np.NaN\n",
    "    return data\n",
    "\n",
    "def get_scoring(groundtruth,predictions,previous):\n",
    "    \"\"\"\n",
    "    Used to get the scoring of the performance of predictions against actual data\n",
    "    \"\"\"\n",
    "    median = np.median(previous)\n",
    "    b_score = 0\n",
    "    p_score = 0\n",
    "    counter = 0\n",
    "    for i in groundtruth:\n",
    "        b_score += ((i - median) ** 2)\n",
    "        p_score += (i - predictions[counter][0]) ** 2\n",
    "        counter += 1\n",
    "    b_score = b_score ** 0.5\n",
    "    p_score = p_score ** 0.5\n",
    "    return b_score,p_score\n",
    "\n",
    "def get_patient_prediction(patient_df,group):\n",
    "    if group == 1:\n",
    "        model = tf.keras.models.load_model(saved_model_group1_10y ,custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "        pred_10y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group1_5y)\n",
    "        pred_5y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group1_2y)\n",
    "        pred_2y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group1_1y)\n",
    "        pred_1y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group1_6m)\n",
    "        pred_6m = model.predict(raw_data)[0][0]\n",
    "\n",
    "    elif group == 2:\n",
    "        model = tf.keras.models.load_model(saved_model_group2_10y ,custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "        pred_10y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group2_5y)\n",
    "        pred_5y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group2_2y)\n",
    "        pred_2y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group2_1y)\n",
    "        pred_1y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group2_6m)\n",
    "        pred_6m = model.predict(raw_data)[0][0]\n",
    "\n",
    "    elif group == 3:\n",
    "        model = tf.keras.models.load_model(saved_model_group3_10y ,custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "        pred_10y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group3_5y)\n",
    "        pred_5y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group3_2y)\n",
    "        pred_2y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group3_1y)\n",
    "        pred_1y = model.predict(raw_data)[0][0]\n",
    "        model.load_weights(saved_model_group3_6m)\n",
    "        pred_6m = model.predict(raw_data)[0][0]\n",
    "\n",
    "    to_return = pd.DataFrame([[pred_6m,pred_1y,pred_2y,pred_5y,pred_10y]],columns = [\"6 months after\",\"1 year after\",\"2 year after\",\"5 years after\",\"10 years after\"])\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical, clinicalOHE, bills_grouped = read_data()\n",
    "\n",
    "remove_indexes = remove_out_of_range(bills_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in groupings\n",
    "x_group1 = pd.read_pickle(best_layer_group1).iloc[:,:-2]\n",
    "x_group2 = pd.read_pickle(best_layer_group2).iloc[:,:-2]\n",
    "x_group3 = pd.read_pickle(best_layer_group3).iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_group1_OHE = loadOHE(x_group1,name=\"group 1_layer 4\")\n",
    "x_group2_OHE = loadOHE(x_group2,name=\"group 2_layer 1\")\n",
    "x_group3_OHE = loadOHE(x_group3,name=\"group 3_layer 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine with bills\n",
    "bills = pd.read_pickle(master_path)\n",
    "y_group1 = get_running_total(bills.loc[x_group1.index.values].iloc[:,-12:])\n",
    "y_group2 = get_running_total(bills.loc[x_group2.index.values].iloc[:,-12:])\n",
    "y_group3 = get_running_total(bills.loc[x_group3.index.values].iloc[:,-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all,y_all = drop_by_index(x_group1_OHE,y_group1,remove_meaningless_data(y_group1))\n",
    "outlier = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5year\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 50:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "\n",
    "X = X_all[~y_all[\"after_5y\"].isna()]\n",
    "y = y_all[~y_all[\"after_5y\"].isna()][[\"after_5y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 500\n",
    "filepath = saved_model_group1_5y\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = saved_model_group1_5y\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "score_actual, score_pred = get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled)\n",
    "print(\"Score (Actual): {}\".format(score_actual[0]))\n",
    "print(\"Score (Prediction): {}\".format(score_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2year\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 10:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "\n",
    "X = X_all[~y_all[\"after_2y\"].isna()]\n",
    "y = y_all[~y_all[\"after_2y\"].isna()][[\"after_2y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 300\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group1_2y.h5\"\n",
    "\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "score_actual, score_pred = get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled)\n",
    "print(\"Score (Actual): {}\".format(score_actual[0]))\n",
    "print(\"Score (Prediction): {}\".format(score_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1year\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "\n",
    "X = X_all[~y_all[\"after_1y\"].isna()]\n",
    "y = y_all[~y_all[\"after_1y\"].isna()][[\"after_1y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 300\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group1_1y.h5\"\n",
    "\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#6month\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "\n",
    "X = X_all[~y_all[\"after_6m\"].isna()]\n",
    "y = y_all[~y_all[\"after_6m\"].isna()][[\"after_6m\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 400\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group1_6m.h5\"\n",
    "\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all,y_all = drop_by_index(x_group2_OHE,y_group2,remove_meaningless_data(y_group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#5year\n",
    "X = X_all[~y_all[\"after_5y\"].isna()]\n",
    "y = y_all[~y_all[\"after_5y\"].isna()][[\"after_5y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 500\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group2_5y.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "\n",
    "#2year\n",
    "    \n",
    "X = X_all[~y_all[\"after_2y\"].isna()]\n",
    "y = y_all[~y_all[\"after_2y\"].isna()][[\"after_2y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "# epochs = 50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group2_2y.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "\n",
    "#1year\n",
    "    \n",
    "X = X_all[~y_all[\"after_1y\"].isna()]\n",
    "y = y_all[~y_all[\"after_1y\"].isna()][[\"after_1y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "# epochs = 50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group2_1y.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "\n",
    "# 6month\n",
    "    \n",
    "X = X_all[~y_all[\"after_6m\"].isna()]\n",
    "y = y_all[~y_all[\"after_6m\"].isna()][[\"after_6m\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "# epochs = 50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group2_6m.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all,y_all = drop_by_index(x_group3_OHE,y_group3,remove_meaningless_data(y_group3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5year\n",
    "    \n",
    "X = X_all[~y_all[\"after_5y\"].isna()]\n",
    "y = y_all[~y_all[\"after_5y\"].isna()][[\"after_5y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs =50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group3_5y.h5\"\n",
    "\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "#2year\n",
    "    \n",
    "X = X_all[~y_all[\"after_2y\"].isna()]\n",
    "y = y_all[~y_all[\"after_2y\"].isna()][[\"after_2y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs =50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group3_2y.h5\"\n",
    "\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "\n",
    "#1year\n",
    "X = X_all[~y_all[\"after_1y\"].isna()]\n",
    "y = y_all[~y_all[\"after_1y\"].isna()][[\"after_1y\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group3_1y.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))\n",
    "\n",
    "#6month\n",
    "X = X_all[~y_all[\"after_6m\"].isna()]\n",
    "y = y_all[~y_all[\"after_6m\"].isna()][[\"after_6m\"]]\n",
    "X,y = drop_by_index(X,y,[])\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = mms.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"C:\\\\SMU_v2\\\\ann\\\\model_group3_6m.h5\"\n",
    "pickle.dump(mms, open(filepath[:-3]+\"_mms.sav\", 'wb'))\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "model.load_weights(filepath)\n",
    "predictions = model.predict(X_test)\n",
    "y_pred_unscaled = mms.inverse_transform(predictions)\n",
    "y_test_unscaled = mms.inverse_transform(y_test)\n",
    "y_actual_unscaled = mms.inverse_transform(y_train)\n",
    "print(get_scoring(y_test_unscaled,y_pred_unscaled,y_actual_unscaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
